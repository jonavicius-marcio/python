{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Comparar diversos modelos </b>  \n",
    "\n",
    "- Naive Bayes \n",
    "- Arvore de Descisão\n",
    "- Randon Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "base = pd.read_csv('census.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré processamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide em previsor e classe \n",
    "previsores = base.iloc[:, 0:14].values\n",
    "classe = base.iloc[:, 14].values\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoder - transforma os atributos de categorico para númerico \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    " \n",
    "labelencoder_previsores = LabelEncoder()\n",
    "previsores[:, 1] = labelencoder_previsores.fit_transform(previsores[:, 1])\n",
    "previsores[:, 3] = labelencoder_previsores.fit_transform(previsores[:, 3])\n",
    "previsores[:, 5] = labelencoder_previsores.fit_transform(previsores[:, 5])\n",
    "previsores[:, 6] = labelencoder_previsores.fit_transform(previsores[:, 6])\n",
    "previsores[:, 7] = labelencoder_previsores.fit_transform(previsores[:, 7])\n",
    "previsores[:, 8] = labelencoder_previsores.fit_transform(previsores[:, 8])\n",
    "previsores[:, 9] = labelencoder_previsores.fit_transform(previsores[:, 9])\n",
    "previsores[:, 13] = labelencoder_previsores.fit_transform(previsores[:, 13])\n",
    "\n",
    "# transformação para a var resposta\n",
    "labelencoder_classe = LabelEncoder()\n",
    "classe = labelencoder_classe.fit_transform(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencorder \n",
    "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])],remainder='passthrough')\n",
    "previsores_onehot = onehotencorder.fit_transform(previsores).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escalonamento\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "previsores_escalonado = scaler.fit_transform(previsores)\n",
    "previsores_Onehot_escalonado=scaler.fit_transform(previsores_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos 4 bases: \n",
    "- previsores: simples\n",
    "- previsores_onehot: com onehoteencode\n",
    "- previsores_escalonado: com escalonamento\n",
    "- previsores_Onehot_escalonado: com onehoteencode e escalonamento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Naive Bayes </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive Bayes\n",
    "\n",
    "def Naive_Bayes (base):\n",
    "    #dividir em base de treino e teste\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(base, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    #treino \n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    classificador = GaussianNB()\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "    ## Validação \n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "\n",
    "    return print(precisao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7952917093142272\n",
      "0.7950870010235415\n",
      "0.8057318321392016\n",
      "0.4767656090071648\n"
     ]
    }
   ],
   "source": [
    "# Simples\n",
    "Naive_Bayes(previsores)\n",
    "# onehoteencode\n",
    "Naive_Bayes(previsores_onehot)\n",
    "# escalonamento\n",
    "Naive_Bayes(previsores_escalonado)\n",
    "# escalonamento\n",
    "Naive_Bayes(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Árvore de Descisão <b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arvore_decisao (base):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(base, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    classificador = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "    \n",
    "    return print(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8128966223132037\n",
      "0.8102354145342886\n",
      "0.8128966223132037\n",
      "0.8104401228249745\n"
     ]
    }
   ],
   "source": [
    "# Simples\n",
    "arvore_decisao(previsores)\n",
    "# onehoteencode\n",
    "arvore_decisao(previsores_onehot)\n",
    "# escalonamento\n",
    "arvore_decisao(previsores_escalonado)\n",
    "# escalonamento\n",
    "arvore_decisao(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Randon Forest <b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def randon_forest (base):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(base, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classificador = RandomForestClassifier(n_estimators=40, criterion='entropy', random_state=0)\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "\n",
    "    return print(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8481064483111566\n",
      "0.8489252814738997\n",
      "0.8483111566018424\n",
      "0.847697031729785\n"
     ]
    }
   ],
   "source": [
    "# Simples\n",
    "randon_forest(previsores)\n",
    "# onehoteencode\n",
    "randon_forest(previsores_onehot)\n",
    "# escalonamento\n",
    "randon_forest(previsores_escalonado)\n",
    "# escalonamento\n",
    "randon_forest(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> SVM </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM (base):   \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(base, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    classificador = SVC(kernel = 'linear', random_state = 1)\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "    \n",
    "    return print(precisao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7d380efb0528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Simples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevisores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# onehoteencode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevisores_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# escalonamento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-fb99d1e8b501>\u001b[0m in \u001b[0;36mSVM\u001b[1;34m(base)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mclassificador\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclassificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevisores_treinamento\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasse_treinamento\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprevisoes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassificador\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprevisores_teste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual_coef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             self._probB, self.fit_status_ = libsvm.fit(\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simples\n",
    "SVM(previsores)\n",
    "# onehoteencode\n",
    "SVM(previsores_onehot)\n",
    "# escalonamento\n",
    "SVM(previsores_escalonado)\n",
    "# escalonamento\n",
    "SVM(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> KNN </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN (base):  \n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    classificador = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "\n",
    "    \n",
    "    return print(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7746161719549641\n",
      "0.7746161719549641\n",
      "0.7746161719549641\n",
      "0.7746161719549641\n"
     ]
    }
   ],
   "source": [
    "# Simples\n",
    "KNN(previsores)\n",
    "# onehoteencode\n",
    "KNN(previsores_onehot)\n",
    "# escalonamento\n",
    "KNN(previsores_escalonado)\n",
    "# escalonamento\n",
    "KNN(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Redes_neurais </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redes_neurais(base):  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    classificador = MLPClassifier(verbose = True, max_iter=1000, tol=0.000010)\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "    print(classificador.out_activation_)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "    \n",
    "    return print(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.73395049\n",
      "Iteration 2, loss = 10.81479603\n",
      "Iteration 3, loss = 10.42011539\n",
      "Iteration 4, loss = 10.51219049\n",
      "Iteration 5, loss = 10.87907506\n",
      "Iteration 6, loss = 11.25361309\n",
      "Iteration 7, loss = 10.83980520\n",
      "Iteration 8, loss = 10.61719202\n",
      "Iteration 9, loss = 10.39734058\n",
      "Iteration 10, loss = 10.26861905\n",
      "Iteration 11, loss = 10.31473479\n",
      "Iteration 12, loss = 10.79312148\n",
      "Iteration 13, loss = 10.69173693\n",
      "Iteration 14, loss = 10.39071148\n",
      "Iteration 15, loss = 10.48638930\n",
      "Iteration 16, loss = 10.42019287\n",
      "Iteration 17, loss = 10.74894990\n",
      "Iteration 18, loss = 9.41113346\n",
      "Iteration 19, loss = 10.18191435\n",
      "Iteration 20, loss = 10.37215991\n",
      "Iteration 21, loss = 10.90573783\n",
      "Iteration 22, loss = 10.15778952\n",
      "Iteration 23, loss = 9.83909667\n",
      "Iteration 24, loss = 10.59373806\n",
      "Iteration 25, loss = 9.94541849\n",
      "Iteration 26, loss = 10.15465006\n",
      "Iteration 27, loss = 10.29355574\n",
      "Iteration 28, loss = 10.79418628\n",
      "Iteration 29, loss = 9.39051043\n",
      "Iteration 30, loss = 10.30133756\n",
      "Iteration 31, loss = 10.52371355\n",
      "Iteration 32, loss = 10.20104182\n",
      "Iteration 33, loss = 9.71201467\n",
      "Iteration 34, loss = 10.14783773\n",
      "Iteration 35, loss = 9.55104524\n",
      "Iteration 36, loss = 10.37702943\n",
      "Iteration 37, loss = 10.74740123\n",
      "Iteration 38, loss = 10.27912628\n",
      "Iteration 39, loss = 10.61103804\n",
      "Iteration 40, loss = 10.35014784\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "logistic\n",
      "0.7330603889457523\n",
      "Iteration 1, loss = 10.65519417\n",
      "Iteration 2, loss = 10.91281542\n",
      "Iteration 3, loss = 10.53323449\n",
      "Iteration 4, loss = 11.07407324\n",
      "Iteration 5, loss = 9.95217921\n",
      "Iteration 6, loss = 9.91958480\n",
      "Iteration 7, loss = 10.50209823\n",
      "Iteration 8, loss = 10.60744480\n",
      "Iteration 9, loss = 10.56224379\n",
      "Iteration 10, loss = 10.61960043\n",
      "Iteration 11, loss = 9.92090918\n",
      "Iteration 12, loss = 10.90394465\n",
      "Iteration 13, loss = 10.35654134\n",
      "Iteration 14, loss = 10.00836157\n",
      "Iteration 15, loss = 10.41879413\n",
      "Iteration 16, loss = 9.84265734\n",
      "Iteration 17, loss = 9.83643724\n",
      "Iteration 18, loss = 10.10988196\n",
      "Iteration 19, loss = 10.28050437\n",
      "Iteration 20, loss = 9.68219317\n",
      "Iteration 21, loss = 10.73205782\n",
      "Iteration 22, loss = 9.95079967\n",
      "Iteration 23, loss = 10.31938258\n",
      "Iteration 24, loss = 10.41193330\n",
      "Iteration 25, loss = 10.63163125\n",
      "Iteration 26, loss = 9.81867403\n",
      "Iteration 27, loss = 10.51780616\n",
      "Iteration 28, loss = 10.05709408\n",
      "Iteration 29, loss = 9.62811961\n",
      "Iteration 30, loss = 10.00250585\n",
      "Iteration 31, loss = 10.18029813\n",
      "Iteration 32, loss = 10.06688791\n",
      "Iteration 33, loss = 9.80614852\n",
      "Iteration 34, loss = 9.95353153\n",
      "Iteration 35, loss = 10.01577522\n",
      "Iteration 36, loss = 9.67064285\n",
      "Iteration 37, loss = 10.61873821\n",
      "Iteration 38, loss = 9.96653901\n",
      "Iteration 39, loss = 9.19565366\n",
      "Iteration 40, loss = 10.63137896\n",
      "Iteration 41, loss = 10.18457795\n",
      "Iteration 42, loss = 9.65320531\n",
      "Iteration 43, loss = 9.91652584\n",
      "Iteration 44, loss = 9.83930875\n",
      "Iteration 45, loss = 9.70557832\n",
      "Iteration 46, loss = 9.42247954\n",
      "Iteration 47, loss = 9.61436048\n",
      "Iteration 48, loss = 10.26076375\n",
      "Iteration 49, loss = 9.46660935\n",
      "Iteration 50, loss = 9.87351051\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "logistic\n",
      "0.7914022517911975\n",
      "Iteration 1, loss = 10.85124503\n",
      "Iteration 2, loss = 10.51215934\n",
      "Iteration 3, loss = 10.69828721\n",
      "Iteration 4, loss = 10.75002244\n",
      "Iteration 5, loss = 10.58367767\n",
      "Iteration 6, loss = 10.49233581\n",
      "Iteration 7, loss = 10.85040730\n",
      "Iteration 8, loss = 10.24402886\n",
      "Iteration 9, loss = 10.46628911\n",
      "Iteration 10, loss = 10.53711724\n",
      "Iteration 11, loss = 10.15503335\n",
      "Iteration 12, loss = 10.45711219\n",
      "Iteration 13, loss = 10.16753412\n",
      "Iteration 14, loss = 10.71324054\n",
      "Iteration 15, loss = 9.51625788\n",
      "Iteration 16, loss = 10.55952728\n",
      "Iteration 17, loss = 10.01178349\n",
      "Iteration 18, loss = 10.50071365\n",
      "Iteration 19, loss = 9.91425340\n",
      "Iteration 20, loss = 11.06426892\n",
      "Iteration 21, loss = 10.34285306\n",
      "Iteration 22, loss = 10.67750841\n",
      "Iteration 23, loss = 10.46685268\n",
      "Iteration 24, loss = 10.41473535\n",
      "Iteration 25, loss = 9.94398913\n",
      "Iteration 26, loss = 10.28092527\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "logistic\n",
      "0.7780962128966223\n",
      "Iteration 1, loss = 11.76868287\n",
      "Iteration 2, loss = 10.47551535\n",
      "Iteration 3, loss = 10.26521934\n",
      "Iteration 4, loss = 10.66665306\n",
      "Iteration 5, loss = 10.34571029\n",
      "Iteration 6, loss = 10.67251013\n",
      "Iteration 7, loss = 10.73534461\n",
      "Iteration 8, loss = 10.26849280\n",
      "Iteration 9, loss = 10.28475081\n",
      "Iteration 10, loss = 10.97091698\n",
      "Iteration 11, loss = 10.58995808\n",
      "Iteration 12, loss = 10.84636187\n",
      "Iteration 13, loss = 10.24232569\n",
      "Iteration 14, loss = 10.07096878\n",
      "Iteration 15, loss = 10.78701945\n",
      "Iteration 16, loss = 10.46536456\n",
      "Iteration 17, loss = 9.87751950\n",
      "Iteration 18, loss = 10.57273640\n",
      "Iteration 19, loss = 10.22170591\n",
      "Iteration 20, loss = 10.04408200\n",
      "Iteration 21, loss = 10.58758797\n",
      "Iteration 22, loss = 9.98008575\n",
      "Iteration 23, loss = 9.82337776\n",
      "Iteration 24, loss = 9.89852334\n",
      "Iteration 25, loss = 10.46570279\n",
      "Iteration 26, loss = 10.13618782\n",
      "Iteration 27, loss = 9.85132747\n",
      "Iteration 28, loss = 10.59040250\n",
      "Iteration 29, loss = 9.91149658\n",
      "Iteration 30, loss = 10.13973268\n",
      "Iteration 31, loss = 9.70729022\n",
      "Iteration 32, loss = 10.11205770\n",
      "Iteration 33, loss = 9.79863012\n",
      "Iteration 34, loss = 9.54035337\n",
      "Iteration 35, loss = 9.74051521\n",
      "Iteration 36, loss = 10.19723181\n",
      "Iteration 37, loss = 9.55858708\n",
      "Iteration 38, loss = 10.47658567\n",
      "Iteration 39, loss = 9.21782888\n",
      "Iteration 40, loss = 9.84132368\n",
      "Iteration 41, loss = 9.83762741\n",
      "Iteration 42, loss = 9.65018178\n",
      "Iteration 43, loss = 9.61636955\n",
      "Iteration 44, loss = 10.12212436\n",
      "Iteration 45, loss = 9.71681268\n",
      "Iteration 46, loss = 10.38281732\n",
      "Iteration 47, loss = 9.80088502\n",
      "Iteration 48, loss = 9.57765441\n",
      "Iteration 49, loss = 10.06147943\n",
      "Iteration 50, loss = 9.49303352\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "logistic\n",
      "0.7942681678607983\n"
     ]
    }
   ],
   "source": [
    "# Simples\n",
    "Redes_neurais(previsores)\n",
    "# onehoteencode\n",
    "Redes_neurais(previsores_onehot)\n",
    "# escalonamento\n",
    "Redes_neurais(previsores_escalonado)\n",
    "# escalonamento\n",
    "Redes_neurais(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Redes_neurais com keras (tensorflow) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redes_neurais_keras (base):  \n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(base, classe, test_size=0.15, random_state=0)\n",
    "\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    classificador = Sequential()\n",
    "    classificador.add(Dense(units = 8, activation = 'relu', input_dim = 14))\n",
    "    classificador.add(Dense(units = 8, activation = 'relu'))\n",
    "    classificador.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    classificador.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    classificador.fit(previsores_treinamento, classe_treinamento, batch_size = 10, epochs = 100)\n",
    "    previsoes = classificador.predict(previsores_teste)\n",
    "    previsoes = (previsoes > 0.5)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    precisao = accuracy_score(classe_teste, previsoes)\n",
    "    matriz = confusion_matrix(classe_teste, previsoes)\n",
    "\n",
    "    return print(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simples\n",
    "Redes_neurais_keras(previsores)\n",
    "# onehoteencode\n",
    "Redes_neurais_keras(previsores_onehot)\n",
    "# escalonamento\n",
    "Redes_neurais_keras(previsores_escalonado)\n",
    "# escalonamento\n",
    "Redes_neurais_keras(previsores_Onehot_escalonado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c218ef86ab8ce6caf09725770f21930b8ce9889d88f1722fcda4fd84da0abe2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
